{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the libraries we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lasio\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import isnan\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "import xgboost as xg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all las files into las objects at once. This will save us time because the read method of lasio is much slower than just calling an already imported las object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasDic = {}\n",
    "for filename in os.listdir(os.getcwd()):\n",
    "    if filename.endswith(\".las\"):\n",
    "        lasDic[filename] = lasio.read(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check file versions\n",
    "for filename in lasDic:\n",
    "    las = lasDic[filename]\n",
    "    print(las.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract well locations\n",
    "#NOTE: well locations are offset by a constant latitude/longitude to make them non-traceable\n",
    "#however, their relative distance is preserved.\n",
    "\n",
    "Lat = []\n",
    "Lon = []\n",
    "for filename in lasDic:\n",
    "    las = lasDic[filename]\n",
    "    Lat.append(las.well['SLAT'].value)\n",
    "    Lon.append(las.well['SLON'].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=Lon,y=Lat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at each file mnemonic and their description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in lasDic:\n",
    "    las = lasDic[filename]\n",
    "    print(las.curves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the times each mnemonic appears in the set of all files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curveCount = {}\n",
    "for filename in lasDic:\n",
    "    las = lasDic[filename]\n",
    "    for item in las.curves.iteritems():\n",
    "        if item[0] in curveCount.keys():\n",
    "            curveCount[item[0]] += 1\n",
    "        else:\n",
    "            curveCount[item[0]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = []\n",
    "for key in curveCount:\n",
    "    if curveCount[key] > 10:\n",
    "        keys.append(key)\n",
    "values = [curveCount[key] for key in keys]\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.bar(keys, values)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of counting the number of times a given mnemonic appears in the entire wells dataset, we can count how many times a given unit of measurement (which is associated with a mnemonic appears). This may be useful because several mnemonics can have the same units and can possibly represent the same physical measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unitCount = {}\n",
    "for filename in lasDic:\n",
    "    las = lasDic[filename]\n",
    "    for item in las.curves.iteritems():\n",
    "        if item[1].unit in unitCount.keys():\n",
    "            unitCount[item[1].unit] += 1\n",
    "        else:\n",
    "            unitCount[item[1].unit] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = []\n",
    "for key in unitCount:\n",
    "    if unitCount[key] > 10:\n",
    "        keys.append(key)\n",
    "values = [unitCount[key] for key in keys]\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.bar(keys, values)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can count how many times a unit appears in the whole dataset but counting it for every well only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unitCount = {}\n",
    "for filename in lasDic:\n",
    "    las = lasDic[filename]\n",
    "    unitset = set()\n",
    "    for item in las.curves.iteritems():\n",
    "        unitset.add(item[1].unit)\n",
    "    for item in unitset:\n",
    "        if item in unitCount.keys():\n",
    "            unitCount[item] += 1\n",
    "        else:\n",
    "            unitCount[item] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = []\n",
    "for key in unitCount:\n",
    "    if unitCount[key] > 10:\n",
    "        keys.append(key)\n",
    "values = [unitCount[key] for key in keys]\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.bar(keys, values)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we extract the actual log data from the las file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "las = lasDic['cbe115c74a89_TGS.las']\n",
    "logs = las.df()\n",
    "\n",
    "f, ax = plt.subplots(nrows=1, ncols=3, figsize=(18, 8), sharey=True)\n",
    "ax[0].plot(logs.GRD, logs.index, color='green')\n",
    "ax[1].plot(logs.DTCO, logs.index, color='black')\n",
    "ax[2].plot(logs.DTSM, logs.index, color='c')\n",
    "for i in range(len(ax)):\n",
    "    ax[i].set_ylim(logs.index[0], logs.index[-1])\n",
    "    ax[i].invert_yaxis()\n",
    "    ax[i].grid()\n",
    "\n",
    "ax[0].set_xlabel(\"GRD\")\n",
    "ax[0].set_xlim(logs.GRD.min(), logs.GRD.max())\n",
    "ax[0].set_ylabel(\"Depth(ft)\")\n",
    "ax[1].set_xlabel(\"DTCO\")\n",
    "ax[1].set_xlim(logs.DTCO.min(), logs.DTCO.max())\n",
    "ax[2].set_xlabel(\"DTSM\")\n",
    "ax[2].set_xlim(logs.DTSM.min(), logs.DTSM.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can map out how two particular mnmonics appears across all wells and across all depths. For example, we can select GRD and GRS and look to see at which wells and at what depth intervals, one of them, both of them, or none of them appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "maxdepth = 25000\n",
    "datamap = np.ndarray(shape=(maxdepth, len(lasDic)), dtype=int)\n",
    "x1 = 'GRD'\n",
    "x2 = 'GRS'\n",
    "wellNo = 0\n",
    "for filename in lasDic:\n",
    "    print(\"on well: {}\".format(filename))\n",
    "    df = lasDic[filename].df()\n",
    "    allmeasures = list(df.columns)\n",
    "    for i in range(maxdepth):\n",
    "        if i not in df.index:\n",
    "            datamap[i,wellNo] = 0 #code for no data at all at that depth\n",
    "            continue\n",
    "        try:\n",
    "            x1_val = df.loc[i, x1]\n",
    "        except:\n",
    "            x1_val = np.nan\n",
    "        try:\n",
    "            x2_val = df.loc[i, x2]\n",
    "        except:\n",
    "            x2_val = np.nan\n",
    "            \n",
    "        if np.isnan(x1_val) and np.isnan(x2_val):\n",
    "            datamap[i,wellNo] = 1 #code for data available at the depth but not x1 or x2\n",
    "        elif not np.isnan(x1_val) and  np.isnan(x2_val):\n",
    "            datamap[i,wellNo] = 2 #code for only x1 available at that depth\n",
    "        elif np.isnan(x1_val) and  not np.isnan(x2_val):\n",
    "            datamap[i,wellNo] = 3 #code for only x2 available at that depth\n",
    "        elif not np.isnan(x1_val) and  not np.isnan(x2_val):\n",
    "            datamap[i,wellNo] = 4 #code for both x1 and x2 available at that depth\n",
    "    wellNo += 1\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "ax=sns.heatmap(datamap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will go through all rows of all wells data and extract those that have both \"GRD\" and \"GRS\" listed for the same depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = 'GRD'\n",
    "x2 = 'GRS'\n",
    "x1List = []\n",
    "x2List = []\n",
    "for filename in lasDic:\n",
    "    allmeasures = list(lasDic[filename].df().columns)\n",
    "    if x1 in allmeasures and x2 in allmeasures:\n",
    "        for index, row in lasDic[filename].df().iterrows():\n",
    "            x1_val = row[allmeasures.index(x1)]\n",
    "            x2_val = row[allmeasures.index(x2)]\n",
    "            if not isnan(x1_val) and not isnan(x2_val):\n",
    "                x1List.append(x1_val)\n",
    "                x2List.append(x2_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x1List, x2List, s=1)\n",
    "plt.xlabel(x1)\n",
    "plt.ylabel(x2)\n",
    "plt.plot([0,600],[0,600], c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's gather x and y data with x being a set of mnemonics that are pretty frequently occured and y as the parameter of interest and train this with a simple xgboost regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlabels = ['GRD', 'DTCO', 'SPR', 'RHOB']\n",
    "ylabels = ['DTSM']\n",
    "x = []\n",
    "y = []\n",
    "for filename in lasDic:\n",
    "    allmeasures = list(lasDic[filename].df().columns)\n",
    "    if all([item in allmeasures for item in xlabels+ylabels]):\n",
    "        for index, row in lasDic[filename].df().iterrows():\n",
    "            inputs = []\n",
    "            [inputs.append(row[allmeasures.index(i)]) for i in xlabels]\n",
    "            outputs = []\n",
    "            [outputs.append(row[allmeasures.index(i)]) for i in ylabels]\n",
    "            if all([not isnan(item) for item in inputs+outputs]):\n",
    "                x.append(inputs)\n",
    "                y.append(outputs)\n",
    "x = np.asarray(x)\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(x,y,test_size=0.3)\n",
    "xgb_r = xg.XGBRegressor(objective='reg:squarederror', n_estimators=10)\n",
    "xgb_r.fit(train_x,train_y)\n",
    "pred = xgb_r.predict(test_x)\n",
    "rmse = np.sqrt(MSE(test_y,pred))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same thing as before except this time we allow datapoints containing NaN values for some of the features to be included in the test/train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlabels = ['GRD', 'DTCO', 'SPR', 'RHOB']\n",
    "ylabels = ['DTSM']\n",
    "x = []\n",
    "y = []\n",
    "for filename in lasDic:\n",
    "    allmeasures = list(lasDic[filename].df().columns)\n",
    "    if all([item in allmeasures for item in ylabels]):\n",
    "        for index, row in lasDic[filename].df().iterrows():\n",
    "            inputs = []\n",
    "            for i in xlabels:\n",
    "                try:\n",
    "                    inputs.append(row[allmeasures.index(i)])\n",
    "                except:\n",
    "                    inputs.append(np.nan)\n",
    "            outputs = []\n",
    "            [outputs.append(row[allmeasures.index(i)]) for i in ylabels]\n",
    "            if all([not isnan(item) for item in outputs]):\n",
    "                x.append(inputs)\n",
    "                y.append(outputs)\n",
    "x = np.asarray(x)\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(x,y,test_size=0.3)\n",
    "xgb_r = xg.XGBRegressor(objective='reg:squarederror', n_estimators=10)\n",
    "xgb_r.fit(train_x,train_y)\n",
    "pred = xgb_r.predict(test_x)\n",
    "rmse = np.sqrt(MSE(test_y,pred))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examplelas = lasDic['5ca18d2db9c6_TGS.las']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
